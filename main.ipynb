{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44af9ff6-021c-40d0-b5d6-9f866c2f9578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cell executed.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os, pandas as pd, numpy as np, pprint as pp, matplotlib.pyplot as plt, json\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "from clustering.clustering_funcs import cluster_glove, merge_cluster_data\n",
    "from phonology.funcs import vectorize_phonology, find_phonology_cosine_similarity_perPhonType, stitch_parts\n",
    "from preprocess.funcs import remove_phonology_duplicate_videos, clear_phonology_df, clear_semantics_df\n",
    "from semantics.funcs import find_semantics_cosine_similarity_pairwise\n",
    "from utility.util_funcs import pandas_pair_signs_alphabetically\n",
    "\n",
    "from itertools import product, combinations\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c5b65-408b-4ed6-8fc4-84589e661e80",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing\n",
    "\n",
    "In this part, we transform the datasets that we have to a format that allows comparisons:\n",
    "\n",
    "- We use pretrained English GloVe vectors (trained on the Wikipedia corpus) to extract semantic similarity between signs\n",
    "- For phonological similarity, we use annotated signs from the Gallaudet Dictionary for ASL, and the SignBank for BSL\n",
    "- The method that we follow does not allow a *direct* comparison between ASL and BSL. We use the available phonological annotations as the basis of our semantic space, which have different entries across the two languages; therefore, the semantic space, while they do share ~500 signs, are not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e06ac-a124-4b33-9ab3-df51ac9af743",
   "metadata": {},
   "source": [
    "## 1.1. Filtering the data\n",
    "\n",
    "In this section, we identify the ASL and the BSL signs that we will be working with.\n",
    "\n",
    "Each sign:\n",
    "- Has to have a phonological transcription\n",
    "- Has to have a semantic vector representation in the GloVe vectors\n",
    "- Must not have duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184a422-ed4f-4fd7-a8cb-e2b645d42c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonRoot_raw = \"data/raw/phonologyData/\"\n",
    "phon_paths_raw = [phonRoot_raw+p for p in os.listdir(phonRoot_raw) if (not p.startswith(\".\")) and (p.endswith(\"xlsx\"))]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about 5 minutes\n",
    "for p in phon_paths_raw:\n",
    "    print(p)\n",
    "#     remove_phonology_duplicate_videos(p)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cd84f-bf32-4880-8736-a793af038684",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonRoot_unique = \"data/transforming/phonologyData/unique_signs/\"\n",
    "phon_paths_uniqueSigns = [phonRoot_unique+p for p in os.listdir(phonRoot_unique) if (not p.startswith(\".\")) and (p.endswith(\"gz\"))]\n",
    "\n",
    "gloveRoot = \"../../../Downloads/glove/\" #replace with path to Glove txt files\n",
    "glovePaths = [gloveRoot+g for g in os.listdir(gloveRoot) if not g.startswith(\".\")]\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b49629-9e07-4431-a252-76bd35e8c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment below if you want to run - Takes about 5 minutes\n",
    "for p in phon_paths_uniqueSigns:\n",
    "    print(p)\n",
    "#     clear_phonology_df(p)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95307d-6e70-4d2c-babf-371fed0bc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment below if you want to run - Takes about 5 minutes\n",
    "for (p, g) in product(phon_paths_uniqueSigns, glovePaths):\n",
    "    print(\"\\n\", p, g)\n",
    "#     clear_semantics_df(p, g)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1bbeb-e313-4145-aa8d-2b484b304608",
   "metadata": {},
   "source": [
    "## 1.2. Transforming and Vectorizing the Phonology Data\n",
    "\n",
    "In this section, we transform the phonology dataframes of ASL and BSL with the following goals in mind:\n",
    "\n",
    "- All signs are lower-cased.\n",
    "- Duplicate signs are removed. Only the first occurrence of a sign is kept. This is a necessary step to avoid skewing the data. For instance, the BSL phonology dataframe has more than 10 entries for the sign MAUVE. We only keep the first occurrence of MAUVE in the order of the rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03361c01-e07f-4956-9e75-bb8e7185cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonRoot_clean = \"data/output/phonologyData/\"\n",
    "phonPaths_clean = [phonRoot_clean+p for p in os.listdir(phonRoot_clean) if not p.startswith(\".\")]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about a couple minutes\n",
    "for p in phonPaths_clean:\n",
    "    print(p)\n",
    "#     vectorize_phonology(p)\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ec9e5-477b-47ed-84e2-bb5117d69853",
   "metadata": {},
   "source": [
    "## 2. Finding Phonological Similarity (Pairwise Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5400f89-4ef6-4b33-9661-0af79dccf5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"FINDING PHONOLOGICAL SIMILARITY FROM VECTORIZED DFs\"\"\"\n",
    "phonRoot_vectorized = \"data/output/vectorizedPhonDFs/\"\n",
    "phonPaths_vectorized = [phonRoot_vectorized+p for p in os.listdir(phonRoot_vectorized) if not p.startswith(\".\")]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about over hour\n",
    "for p in phonPaths_vectorized:\n",
    "    print(p)\n",
    "#     find_phonology_cosine_similarity_perPhonType(p)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562db36-7067-4a7a-b292-bbe21d9606c0",
   "metadata": {},
   "source": [
    "## 3. Finding Semantic Similarity (Pairwise Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2384b4-cdaf-42e7-bf8f-e922004951a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "semPath_root = \"data/output/semanticsData/\"\n",
    "semPaths = [semPath_root+p for p in os.listdir(semPath_root) if not p.startswith(\".\")]\n",
    "\n",
    "#Uncomment below if you want to run - Takes over an hour\n",
    "for p in semPaths:\n",
    "    print(p)\n",
    "#     find_semantics_cosine_similarity_pairwise(p)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497e6cf-85d9-4414-87de-c09cbebcfc91",
   "metadata": {},
   "source": [
    "## 4. Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3189d-0e49-425e-84bf-a1872d3065f3",
   "metadata": {},
   "source": [
    "## 4.1. Pairwise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39142d-2193-43db-8438-b20abf5ebdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STITCHING TOGETHER PHONOLOGICAL SIMILARITY CSVs\"\"\"\n",
    "\n",
    "masterPath = \"data/output/vectorizedPhonDFs_Stitched_DFs/\"\n",
    "if not os.path.exists(masterPath):\n",
    "    os.makedirs(masterPath)\n",
    "    \n",
    "languages = [\"ASL\", \"BSL\"]\n",
    "phonTypes = [\"ENTIRE\", \"LOC\", \"MOV\", \"HS\"]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about 5 minutes\n",
    "for language, phonType in product(languages, phonTypes):\n",
    "    print(language, phonType)\n",
    "    path = \"data/output/vectorized_PhonSim/\"+language+\"/\"+phonType+\"/\"\n",
    "    df = stitch_parts(path)\n",
    "    df.to_csv(masterPath+language+\"_\"+phonType+\".csv.gz\", compression=\"gzip\",index=False)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a2836-15b8-440f-bc2a-75e424ac47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STITCHING TOGETHER SEMANTIC SIMILARITY CSVs\"\"\"\n",
    "\n",
    "masterPath = \"data/output/SemSim_StitchedDFs/\"\n",
    "if not os.path.exists(masterPath):\n",
    "    os.makedirs(masterPath)\n",
    "    \n",
    "languages = [\"ASL\", \"BSL\"]\n",
    "dims = [\"50d\", \"100d\", \"200d\", \"300d\"]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about 5 minutes\n",
    "for language, dim in product(languages, dims):\n",
    "    print(language, dim)\n",
    "    path = \"data/output/SemSim/\"+language+\"/\"+dim+\"/\"\n",
    "    df = stitch_parts(path)\n",
    "    df.to_csv(masterPath+language+\"_\"+dim+\".csv.gz\", compression=\"gzip\",index=False)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84fdb4-3df2-4fa9-b653-858e23795ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PAIRWISE CORRELATIONS ANALYSIS\"\"\"\n",
    "\n",
    "masterPhonPath = \"data/output/vectorizedPhonDFs_Stitched_DFs/\"\n",
    "masterSemPath = \"data/output/SemSim_StitchedDFs/\"\n",
    "semPaths = [masterSemPath + p for p in os.listdir(masterSemPath) if not p.startswith(\".\")]\n",
    "phonPaths = [masterPhonPath + p for p in os.listdir(masterPhonPath) if not p.startswith(\".\")]\n",
    "\n",
    "#Creating a dataframe to store pairwise calculations\n",
    "df_pairwise_calculations = pd.DataFrame(columns=[\"phonType\", \"semDimension\", \"language\", \"pearson_r\", \"p-value\"], index=range(32))\n",
    "\n",
    "df_Limit = None\n",
    "i = -1\n",
    "for s, p in list(product(semPaths, phonPaths))[:df_Limit]:\n",
    "    language_sem = s.split(\"/\")[-1].split(\"_\")[0]\n",
    "    language_phon = p.split(\"/\")[-1].split(\"_\")[0]\n",
    "    \n",
    "    if language_sem == language_phon:\n",
    "        i += 1\n",
    "        \n",
    "        language = language_sem\n",
    "        dim = s.split(\"/\")[-1].split(\"_\")[1].split(\".\")[0]\n",
    "        phonType = p.split(\"/\")[-1].split(\"_\")[1].split(\".\")[0]\n",
    "        \n",
    "        print(i, language, dim, phonType)\n",
    "        \n",
    "        df_pairwise_calculations.iloc[i][\"language\"] = language\n",
    "        df_pairwise_calculations.iloc[i][\"phonType\"] = phonType\n",
    "        df_pairwise_calculations.iloc[i][\"semDimension\"] = dim\n",
    "        \n",
    "        #Loading CSVs\n",
    "        phonDF = pd.read_csv(p)\n",
    "        semDF = pd.read_csv(s)\n",
    "        \n",
    "        phonDF[\"paired\"] = phonDF.apply(lambda x: pandas_pair_signs_alphabetically(x), axis=1)\n",
    "        semDF[\"paired\"] = semDF.apply(lambda x: pandas_pair_signs_alphabetically(x), axis=1)\n",
    "        \n",
    "        phonDF = phonDF.drop([\"s1\", \"s2\"], axis=1).set_index(\"paired\").sort_index()\n",
    "        semDF = semDF.drop([\"s1\", \"s2\"], axis=1).set_index(\"paired\").sort_index()\n",
    "        \n",
    "        \n",
    "        if all(phonDF.index == semDF.index):\n",
    "            print(\"All indices match.\\n\\n\")\n",
    "        else:\n",
    "            print(\"INDICES DO NOT MATCH.\\n\\n\")\n",
    "            \n",
    "        #Calculate correlations\n",
    "        df_pairwise_calculations.iloc[i][\"pearson_r\"], df_pairwise_calculations.iloc[i][\"p-value\"] = pearsonr(phonDF[phonType+\"_cosineSim\"], semDF[\"sem_cosineSim\"])\n",
    "        \n",
    "        \"\"\"ADD VISUALIZATIONS HERE\"\"\"\n",
    "        \n",
    "        del phonDF\n",
    "        del semDF\n",
    "        \n",
    "df_pairwise_calculations.to_csv(\"results/pairwise/pairwise_results.csv\", index=False)\n",
    "df_pairwise_calculations\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c886d-88d2-4d90-a5f4-a72fe1ced2b4",
   "metadata": {},
   "source": [
    "### Results of the Pairwise Analysis:\n",
    "\n",
    "There is no apparent linear relationship between phonological similarity (as measured by the additive inverse of cosine distance between two signs in space that are vectorized) and semantic similarity (measured using the same cosine method as phonological similarity -- except we use GloVe vectors pretrained on the Wikipedia corpus).\n",
    "\n",
    "Phonology is arbitrary when the lexicon of an SL is taken as a whole.\n",
    "\n",
    "This brings us to our next analysis: Hierarchical Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec65882-d7b1-4a35-831a-44ebef30cae4",
   "metadata": {},
   "source": [
    "## 4.2. Hierarchical Clustering Analysis\n",
    "\n",
    "In this section, we raise the question that if there is no linear relationship in the phonology and semantics of pairs of signs in a semantically ***unorganized*** lexicon of an SL, can we find relationships betweeen pairs of signs within clusters of semantically related signs? \n",
    "\n",
    "1. We first cluster signs in a semantic vector space using agglomerative hierarchical clustering\n",
    "2. We then look for pairwise relations between pairs of signs within individual clusters.\n",
    "3. This dramatically reduces the number of sign pairs that we study, as the pairing process does not cross cluster boundaries in a given semantic vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0584b-dd89-427c-a4ca-a2079d5064ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "semPath_root = \"data/output/semanticsData/\"\n",
    "semPaths = [semPath_root+p for p in os.listdir(semPath_root) if not p.startswith(\".\")]\n",
    "\n",
    "languages = [\"ASL\", \"BSL\"]\n",
    "dims = [\"50d\", \"100d\", \"200d\", \"300d\"]\n",
    "\n",
    "heightRange = range(0,100)\n",
    "tuples = product(languages, dims)\n",
    "columns = pd.MultiIndex.from_tuples(tuples, names=[\"language\", \"dim\"])\n",
    "clusterN_df = pd.DataFrame(index=heightRange, columns=columns)\n",
    "\n",
    "\"\"\"CLUSTERING -- pruning heights 0% through 100%\"\"\"\n",
    "for language in languages:\n",
    "    for dim in dims:\n",
    "        masterPath = \"results/clustering/clusterIDs/\"+language+'/'+dim+'/'\n",
    "        \n",
    "        if not os.path.exists(masterPath):\n",
    "            os.makedirs(masterPath)\n",
    "\n",
    "        p = semPath_root+language+\"_Semantics_\"+dim+\"_clean.csv.gz\"\n",
    "        print(p)\n",
    "        \n",
    "        clusterLabels_dict = {}\n",
    "        for heit in heightRange:\n",
    "            \n",
    "            if heit%25 == 0:\n",
    "                print(heit)\n",
    "                \n",
    "            signs, silhouette, clusterLabels, clusters_len = cluster_glove(p, height=heit)\n",
    "            \n",
    "            if  1 < clusters_len < len(signs):\n",
    "                clusterN_df[(language, dim)].loc[heit] = clusters_len\n",
    "                temp_dict = {sign:cluster for sign, cluster in zip(signs, clusterLabels)}\n",
    "                height_key = \"height_\"+str(heit).zfill(3)\n",
    "                clusterLabels_dict[height_key] = {}\n",
    "                clusterLabels_dict[height_key][\"clusters\"] = {\"C\"+str(key).zfill(4): [value for value, check_key in temp_dict.items() if check_key==key] for key in temp_dict.values()}\n",
    "                clusterLabels_dict[height_key][\"silhouette_score\"] = silhouette\n",
    "                \n",
    "        with open(masterPath+language+\"_\"+dim+\"_\"+\"clusterIDs.json\", \"w\") as outfile:\n",
    "             json.dump(clusterLabels_dict, outfile)\n",
    "            \n",
    "\n",
    "clusterN_df = clusterN_df.reset_index()\n",
    "clusterN_df = clusterN_df.rename(columns={\"index\":\"height\"})\n",
    "clusterN_df.to_csv(\"results/clustering/clusterN_by_height.csv\", index=True)\n",
    "clusterN_df\n",
    "\n",
    "print(\"\\n\\nCell executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138ca40-5b08-4c99-a4fd-e063471659ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"ASL\", \"BSL\"]\n",
    "dims = [\"50d\", \"100d\", \"200d\", \"300d\"]\n",
    "\n",
    "masterPath = \"results/clustering/clusterIDs/\"\n",
    "\n",
    "silhous = pd.DataFrame(columns = [\"language\", \"dim\", \"prune_height\", \"cluster_N\", \"silhouette_score\"], index=range(len(languages)*len(dims)*100))\n",
    "i = 0\n",
    "for language in languages:\n",
    "    for dim in dims:\n",
    "        read_json_path = masterPath+language+\"/\"+dim+\"/\"+language+\"_\"+dim+\"_clusterIDs.json\"\n",
    "        print(read_json_path)\n",
    "        with open(read_json_path, \"r\") as read_file:\n",
    "            clusterLabels = json.load(read_file)\n",
    "        \n",
    "        for heit in clusterLabels.keys():\n",
    "            height = int(heit.split(\"_\")[1])\n",
    "            silhous.iloc[i][\"language\"] = language\n",
    "            silhous.iloc[i][\"dim\"] = dim\n",
    "            silhous.iloc[i][\"prune_height\"] = height\n",
    "            silhous.iloc[i][\"cluster_N\"] = len(clusterLabels[heit][\"clusters\"])\n",
    "            silhous.iloc[i][\"silhouette_score\"] = clusterLabels[heit][\"silhouette_score\"]\n",
    "            i+= 1\n",
    "        \n",
    "    \n",
    "silhous = silhous.dropna()\n",
    "silhous.head()\n",
    "silhous.to_csv(\"results/clustering/clustering_silhouette_scores.csv\", index=False)\n",
    "\n",
    "print(\"\\n\\nCell executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2fe9f-aa3d-4e53-89b9-8f0343592df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the VSMs where clustering quality is highest (the higher the silhouette score the better clustering quality)\n",
    "silhous.groupby([\"language\", \"dim\"])[\"silhouette_score\"].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37306aaa-1f97-48f9-ace4-88a1658071ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "languages = [\"ASL\", \"BSL\"]\n",
    "dims = [\"50d\", \"100d\", \"200d\", \"300d\"]\n",
    "\n",
    "masterPath = \"results/clustering/clusterIDs/\"\n",
    "\n",
    "lowerBound = 2\n",
    "upperBound = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7c2b3-1a6e-43d0-a300-83e1d6b7ee94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    for dim in dims:\n",
    "        read_json_path = masterPath+language+\"/\"+dim+\"/\"+language+\"_\"+dim+\"_clusterIDs.json\"\n",
    "        print(read_json_path)\n",
    "        with open(read_json_path, \"r\") as read_file:\n",
    "            clusterLabels = json.load(read_file)\n",
    "            \n",
    "        masterOutPath = \"results/clustering/signPairs_byCluster/\"+language+\"/\"+dim+\"/\"\n",
    "        if not os.path.exists(masterOutPath):\n",
    "            os.makedirs(masterOutPath)\n",
    "        \n",
    "        for heit in clusterLabels.keys():\n",
    "            height = int(heit.split(\"_\")[1])\n",
    "            \n",
    "            height_condition = (lowerBound-1)<height<(upperBound)\n",
    "            #The range above is obtained from the silhouette scores.\n",
    "            #All VSMs have the max silhouette score between heights 4 and 7.\n",
    "            #So we only look at those VSMs where cluster validity is better:\n",
    "            \n",
    "            if height_condition:\n",
    "                signPairs = {cluster: [sorted((x,y)) for (x,y) in combinations(clusterLabels[heit][\"clusters\"][cluster],2)] for cluster in clusterLabels[heit][\"clusters\"] if len(clusterLabels[heit][\"clusters\"][cluster]) > 1}\n",
    "                out_json_path = masterOutPath+language+\"_\"+dim+\"_height\"+heit+\"_signPairs_byCluster.json\"\n",
    "                with open(out_json_path,'w') as outfile:\n",
    "                    json.dump(signPairs, outfile)\n",
    "            \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c99fa74-1c88-4ec6-8593-27389fb36e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all cluster information into one dataframe and outputting to outPath\n",
    "outPath = \"results/clustering/allClusters/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65ad8d-9b5c-44f0-a62b-f16c14a2ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can take anywhere from 30 minutes to a COUPLE OF DAYS\n",
    "# depending on the lower and upper bounds above. \n",
    "\n",
    "#Higher pruning height values are more complex to compute. More complex in the sense that\n",
    "#they yield fewer but heavily populated clusters -- the number of sign pairs in each of these\n",
    "#clusters (if the number of signs in a cluster C is N, then the number of sign pairs in cluster C\n",
    "#is N!/(r!(N-r)!) where r=2 because we find pairs) is much higher than if the pruning height was lower.\n",
    "#Lower pruning height values yield a higher number of clusters where clusters have fewer members.\n",
    "#To see number of clusters per pruning height in the 8 VSMs we have (ASL, BSL X 50,100,200,300D)\n",
    "#check out '/results/clustering/clusterN_by_height.csv'\n",
    "\n",
    "#Uncomment the line below to run.\n",
    "# allClusters = merge_cluster_data(outPath)\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f444c45-ab6e-43e4-9dc7-c6b5194cfdf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allClusters = pd.DataFrame()\n",
    "allClusters_paths = [outPath+x for x in os.listdir(outPath) if not x.startswith(\".\")]\n",
    "for path in allClusters_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    allClusters = allClusters.append(df, ignore_index=True)\n",
    "\n",
    "allClusters.head()\n",
    "print(len(allClusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c2027b-8dfc-4099-a46b-1bd6b64ff933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL 50d 2 ENTIRE\n",
      "ASL 50d 2 HS\n",
      "ASL 50d 2 MOV\n",
      "ASL 50d 2 LOC\n",
      "ASL 50d 3 ENTIRE\n",
      "ASL 50d 3 HS\n",
      "ASL 50d 3 MOV\n",
      "ASL 50d 3 LOC\n",
      "ASL 50d 4 ENTIRE\n",
      "ASL 50d 4 HS\n",
      "ASL 50d 4 MOV\n",
      "ASL 50d 4 LOC\n",
      "ASL 50d 5 ENTIRE\n",
      "ASL 50d 5 HS\n",
      "ASL 50d 5 MOV\n",
      "ASL 50d 5 LOC\n",
      "ASL 50d 6 ENTIRE\n",
      "ASL 50d 6 HS\n",
      "ASL 50d 6 MOV\n",
      "ASL 50d 6 LOC\n",
      "ASL 50d 7 ENTIRE\n",
      "ASL 50d 7 HS\n",
      "ASL 50d 7 MOV\n",
      "ASL 50d 7 LOC\n",
      "ASL 50d 8 ENTIRE\n",
      "ASL 50d 8 HS\n",
      "ASL 50d 8 MOV\n",
      "ASL 50d 8 LOC\n",
      "ASL 50d 9 ENTIRE\n",
      "ASL 50d 9 HS\n",
      "ASL 50d 9 MOV\n",
      "ASL 50d 9 LOC\n",
      "ASL 50d 10 ENTIRE\n",
      "ASL 50d 10 HS\n",
      "ASL 50d 10 MOV\n",
      "ASL 50d 10 LOC\n",
      "ASL 50d 11 ENTIRE\n",
      "ASL 50d 11 HS\n",
      "ASL 50d 11 MOV\n",
      "ASL 50d 11 LOC\n",
      "ASL 50d 12 ENTIRE\n",
      "ASL 50d 12 HS\n",
      "ASL 50d 12 MOV\n",
      "ASL 50d 12 LOC\n",
      "ASL 50d 13 ENTIRE\n",
      "ASL 50d 13 HS\n",
      "ASL 50d 13 MOV\n",
      "ASL 50d 13 LOC\n",
      "ASL 50d 14 ENTIRE\n",
      "ASL 50d 14 HS\n",
      "ASL 50d 14 MOV\n",
      "ASL 50d 14 LOC\n",
      "ASL 50d 15 ENTIRE\n",
      "ASL 50d 15 HS\n",
      "ASL 50d 15 MOV\n",
      "ASL 50d 15 LOC\n",
      "ASL 50d 16 ENTIRE\n",
      "ASL 50d 16 HS\n",
      "ASL 50d 16 MOV\n",
      "ASL 50d 16 LOC\n",
      "ASL 50d 17 ENTIRE\n",
      "ASL 50d 17 HS\n",
      "ASL 50d 17 MOV\n",
      "ASL 50d 17 LOC\n",
      "ASL 50d 18 ENTIRE\n",
      "ASL 50d 18 HS\n",
      "ASL 50d 18 MOV\n",
      "ASL 50d 18 LOC\n",
      "ASL 50d 19 ENTIRE\n",
      "ASL 50d 19 HS\n",
      "ASL 50d 19 MOV\n",
      "ASL 50d 19 LOC\n",
      "ASL 50d 20 ENTIRE\n",
      "ASL 50d 20 HS\n",
      "ASL 50d 20 MOV\n",
      "ASL 50d 20 LOC\n",
      "ASL 50d 21 ENTIRE\n",
      "ASL 50d 21 HS\n",
      "ASL 50d 21 MOV\n",
      "ASL 50d 21 LOC\n",
      "ASL 50d 22 ENTIRE\n",
      "ASL 50d 22 HS\n",
      "ASL 50d 22 MOV\n",
      "ASL 50d 22 LOC\n",
      "ASL 50d 23 ENTIRE\n",
      "ASL 50d 23 HS\n",
      "ASL 50d 23 MOV\n",
      "ASL 50d 23 LOC\n",
      "ASL 50d 24 ENTIRE\n",
      "ASL 50d 24 HS\n",
      "ASL 50d 24 MOV\n",
      "ASL 50d 24 LOC\n",
      "ASL 50d 25 ENTIRE\n",
      "ASL 50d 25 HS\n",
      "ASL 50d 25 MOV\n",
      "ASL 50d 25 LOC\n",
      "ASL 50d 26 ENTIRE\n",
      "ASL 50d 26 HS\n",
      "ASL 50d 26 MOV\n",
      "ASL 50d 26 LOC\n",
      "ASL 50d 27 ENTIRE\n",
      "ASL 50d 27 HS\n",
      "ASL 50d 27 MOV\n",
      "ASL 50d 27 LOC\n",
      "ASL 50d 28 ENTIRE\n",
      "ASL 50d 28 HS\n",
      "ASL 50d 28 MOV\n",
      "ASL 50d 28 LOC\n",
      "ASL 50d 29 ENTIRE\n",
      "ASL 50d 29 HS\n",
      "ASL 50d 29 MOV\n",
      "ASL 50d 29 LOC\n",
      "ASL 50d 30 ENTIRE\n",
      "ASL 50d 30 HS\n",
      "ASL 50d 30 MOV\n",
      "ASL 50d 30 LOC\n",
      "ASL 100d 2 ENTIRE\n",
      "ASL 100d 2 HS\n",
      "ASL 100d 2 MOV\n",
      "ASL 100d 2 LOC\n",
      "ASL 100d 3 ENTIRE\n",
      "ASL 100d 3 HS\n",
      "ASL 100d 3 MOV\n",
      "ASL 100d 3 LOC\n",
      "ASL 100d 4 ENTIRE\n",
      "ASL 100d 4 HS\n",
      "ASL 100d 4 MOV\n",
      "ASL 100d 4 LOC\n",
      "ASL 100d 5 ENTIRE\n",
      "ASL 100d 5 HS\n",
      "ASL 100d 5 MOV\n",
      "ASL 100d 5 LOC\n",
      "ASL 100d 6 ENTIRE\n",
      "ASL 100d 6 HS\n",
      "ASL 100d 6 MOV\n",
      "ASL 100d 6 LOC\n",
      "ASL 100d 7 ENTIRE\n",
      "ASL 100d 7 HS\n",
      "ASL 100d 7 MOV\n",
      "ASL 100d 7 LOC\n",
      "ASL 100d 8 ENTIRE\n",
      "ASL 100d 8 HS\n",
      "ASL 100d 8 MOV\n",
      "ASL 100d 8 LOC\n",
      "ASL 100d 9 ENTIRE\n",
      "ASL 100d 9 HS\n",
      "ASL 100d 9 MOV\n",
      "ASL 100d 9 LOC\n",
      "ASL 100d 10 ENTIRE\n",
      "ASL 100d 10 HS\n",
      "ASL 100d 10 MOV\n",
      "ASL 100d 10 LOC\n",
      "ASL 100d 11 ENTIRE\n",
      "ASL 100d 11 HS\n",
      "ASL 100d 11 MOV\n",
      "ASL 100d 11 LOC\n",
      "ASL 100d 12 ENTIRE\n",
      "ASL 100d 12 HS\n",
      "ASL 100d 12 MOV\n",
      "ASL 100d 12 LOC\n",
      "ASL 100d 13 ENTIRE\n",
      "ASL 100d 13 HS\n",
      "ASL 100d 13 MOV\n",
      "ASL 100d 13 LOC\n",
      "ASL 100d 14 ENTIRE\n",
      "ASL 100d 14 HS\n",
      "ASL 100d 14 MOV\n",
      "ASL 100d 14 LOC\n",
      "ASL 100d 15 ENTIRE\n",
      "ASL 100d 15 HS\n",
      "ASL 100d 15 MOV\n",
      "ASL 100d 15 LOC\n",
      "ASL 100d 16 ENTIRE\n",
      "ASL 100d 16 HS\n",
      "ASL 100d 16 MOV\n",
      "ASL 100d 16 LOC\n",
      "ASL 100d 17 ENTIRE\n",
      "ASL 100d 17 HS\n",
      "ASL 100d 17 MOV\n",
      "ASL 100d 17 LOC\n",
      "ASL 100d 18 ENTIRE\n",
      "ASL 100d 18 HS\n",
      "ASL 100d 18 MOV\n",
      "ASL 100d 18 LOC\n",
      "ASL 100d 19 ENTIRE\n",
      "ASL 100d 19 HS\n",
      "ASL 100d 19 MOV\n",
      "ASL 100d 19 LOC\n",
      "ASL 100d 20 ENTIRE\n",
      "ASL 100d 20 HS\n",
      "ASL 100d 20 MOV\n",
      "ASL 100d 20 LOC\n",
      "ASL 100d 21 ENTIRE\n",
      "ASL 100d 21 HS\n",
      "ASL 100d 21 MOV\n",
      "ASL 100d 21 LOC\n",
      "ASL 100d 22 ENTIRE\n",
      "ASL 100d 22 HS\n",
      "ASL 100d 22 MOV\n",
      "ASL 100d 22 LOC\n",
      "ASL 100d 23 ENTIRE\n",
      "ASL 100d 23 HS\n",
      "ASL 100d 23 MOV\n",
      "ASL 100d 23 LOC\n",
      "ASL 100d 24 ENTIRE\n",
      "ASL 100d 24 HS\n",
      "ASL 100d 24 MOV\n",
      "ASL 100d 24 LOC\n",
      "ASL 100d 25 ENTIRE\n",
      "ASL 100d 25 HS\n",
      "ASL 100d 25 MOV\n",
      "ASL 100d 25 LOC\n",
      "ASL 100d 26 ENTIRE\n",
      "ASL 100d 26 HS\n",
      "ASL 100d 26 MOV\n",
      "ASL 100d 26 LOC\n",
      "ASL 100d 27 ENTIRE\n",
      "ASL 100d 27 HS\n",
      "ASL 100d 27 MOV\n",
      "ASL 100d 27 LOC\n",
      "ASL 100d 28 ENTIRE\n",
      "ASL 100d 28 HS\n",
      "ASL 100d 28 MOV\n",
      "ASL 100d 28 LOC\n",
      "ASL 100d 29 ENTIRE\n",
      "ASL 100d 29 HS\n",
      "ASL 100d 29 MOV\n",
      "ASL 100d 29 LOC\n",
      "ASL 100d 30 ENTIRE\n",
      "ASL 100d 30 HS\n",
      "ASL 100d 30 MOV\n",
      "ASL 100d 30 LOC\n",
      "ASL 200d 2 ENTIRE\n",
      "ASL 200d 2 HS\n",
      "ASL 200d 2 MOV\n",
      "ASL 200d 2 LOC\n",
      "ASL 200d 3 ENTIRE\n",
      "ASL 200d 3 HS\n",
      "ASL 200d 3 MOV\n",
      "ASL 200d 3 LOC\n",
      "ASL 200d 4 ENTIRE\n",
      "ASL 200d 4 HS\n",
      "ASL 200d 4 MOV\n",
      "ASL 200d 4 LOC\n",
      "ASL 200d 5 ENTIRE\n",
      "ASL 200d 5 HS\n",
      "ASL 200d 5 MOV\n",
      "ASL 200d 5 LOC\n",
      "ASL 200d 6 ENTIRE\n",
      "ASL 200d 6 HS\n",
      "ASL 200d 6 MOV\n",
      "ASL 200d 6 LOC\n",
      "ASL 200d 7 ENTIRE\n",
      "ASL 200d 7 HS\n",
      "ASL 200d 7 MOV\n",
      "ASL 200d 7 LOC\n",
      "ASL 200d 8 ENTIRE\n",
      "ASL 200d 8 HS\n",
      "ASL 200d 8 MOV\n",
      "ASL 200d 8 LOC\n",
      "ASL 200d 9 ENTIRE\n",
      "ASL 200d 9 HS\n",
      "ASL 200d 9 MOV\n",
      "ASL 200d 9 LOC\n",
      "ASL 200d 10 ENTIRE\n",
      "ASL 200d 10 HS\n",
      "ASL 200d 10 MOV\n",
      "ASL 200d 10 LOC\n",
      "ASL 200d 11 ENTIRE\n",
      "ASL 200d 11 HS\n",
      "ASL 200d 11 MOV\n",
      "ASL 200d 11 LOC\n",
      "ASL 200d 12 ENTIRE\n",
      "ASL 200d 12 HS\n",
      "ASL 200d 12 MOV\n",
      "ASL 200d 12 LOC\n",
      "ASL 200d 13 ENTIRE\n",
      "ASL 200d 13 HS\n",
      "ASL 200d 13 MOV\n",
      "ASL 200d 13 LOC\n",
      "ASL 200d 14 ENTIRE\n",
      "ASL 200d 14 HS\n",
      "ASL 200d 14 MOV\n",
      "ASL 200d 14 LOC\n",
      "ASL 200d 15 ENTIRE\n",
      "ASL 200d 15 HS\n",
      "ASL 200d 15 MOV\n",
      "ASL 200d 15 LOC\n",
      "ASL 200d 16 ENTIRE\n",
      "ASL 200d 16 HS\n",
      "ASL 200d 16 MOV\n",
      "ASL 200d 16 LOC\n",
      "ASL 200d 17 ENTIRE\n",
      "ASL 200d 17 HS\n",
      "ASL 200d 17 MOV\n",
      "ASL 200d 17 LOC\n",
      "ASL 200d 18 ENTIRE\n",
      "ASL 200d 18 HS\n",
      "ASL 200d 18 MOV\n",
      "ASL 200d 18 LOC\n",
      "ASL 200d 19 ENTIRE\n",
      "ASL 200d 19 HS\n",
      "ASL 200d 19 MOV\n",
      "ASL 200d 19 LOC\n",
      "ASL 200d 20 ENTIRE\n",
      "ASL 200d 20 HS\n",
      "ASL 200d 20 MOV\n",
      "ASL 200d 20 LOC\n",
      "ASL 200d 21 ENTIRE\n",
      "ASL 200d 21 HS\n",
      "ASL 200d 21 MOV\n",
      "ASL 200d 21 LOC\n",
      "ASL 200d 22 ENTIRE\n",
      "ASL 200d 22 HS\n",
      "ASL 200d 22 MOV\n",
      "ASL 200d 22 LOC\n",
      "ASL 200d 23 ENTIRE\n",
      "ASL 200d 23 HS\n",
      "ASL 200d 23 MOV\n",
      "ASL 200d 23 LOC\n",
      "ASL 200d 24 ENTIRE\n",
      "ASL 200d 24 HS\n",
      "ASL 200d 24 MOV\n",
      "ASL 200d 24 LOC\n",
      "ASL 200d 25 ENTIRE\n",
      "ASL 200d 25 HS\n",
      "ASL 200d 25 MOV\n",
      "ASL 200d 25 LOC\n",
      "ASL 200d 26 ENTIRE\n",
      "ASL 200d 26 HS\n",
      "ASL 200d 26 MOV\n",
      "ASL 200d 26 LOC\n",
      "ASL 200d 27 ENTIRE\n",
      "ASL 200d 27 HS\n",
      "ASL 200d 27 MOV\n",
      "ASL 200d 27 LOC\n",
      "ASL 200d 28 ENTIRE\n",
      "ASL 200d 28 HS\n",
      "ASL 200d 28 MOV\n",
      "ASL 200d 28 LOC\n",
      "ASL 200d 29 ENTIRE\n",
      "ASL 200d 29 HS\n",
      "ASL 200d 29 MOV\n",
      "ASL 200d 29 LOC\n",
      "ASL 200d 30 ENTIRE\n",
      "ASL 200d 30 HS\n",
      "ASL 200d 30 MOV\n",
      "ASL 200d 30 LOC\n",
      "ASL 300d 2 ENTIRE\n",
      "ASL 300d 2 HS\n",
      "ASL 300d 2 MOV\n",
      "ASL 300d 2 LOC\n",
      "ASL 300d 3 ENTIRE\n",
      "ASL 300d 3 HS\n",
      "ASL 300d 3 MOV\n",
      "ASL 300d 3 LOC\n",
      "ASL 300d 4 ENTIRE\n",
      "ASL 300d 4 HS\n",
      "ASL 300d 4 MOV\n",
      "ASL 300d 4 LOC\n",
      "ASL 300d 5 ENTIRE\n",
      "ASL 300d 5 HS\n",
      "ASL 300d 5 MOV\n",
      "ASL 300d 5 LOC\n",
      "ASL 300d 6 ENTIRE\n",
      "ASL 300d 6 HS\n",
      "ASL 300d 6 MOV\n",
      "ASL 300d 6 LOC\n",
      "ASL 300d 7 ENTIRE\n",
      "ASL 300d 7 HS\n",
      "ASL 300d 7 MOV\n",
      "ASL 300d 7 LOC\n",
      "ASL 300d 8 ENTIRE\n",
      "ASL 300d 8 HS\n",
      "ASL 300d 8 MOV\n",
      "ASL 300d 8 LOC\n",
      "ASL 300d 9 ENTIRE\n",
      "ASL 300d 9 HS\n",
      "ASL 300d 9 MOV\n",
      "ASL 300d 9 LOC\n",
      "ASL 300d 10 ENTIRE\n",
      "ASL 300d 10 HS\n",
      "ASL 300d 10 MOV\n",
      "ASL 300d 10 LOC\n",
      "ASL 300d 11 ENTIRE\n",
      "ASL 300d 11 HS\n",
      "ASL 300d 11 MOV\n",
      "ASL 300d 11 LOC\n",
      "ASL 300d 12 ENTIRE\n",
      "ASL 300d 12 HS\n",
      "ASL 300d 12 MOV\n",
      "ASL 300d 12 LOC\n",
      "ASL 300d 13 ENTIRE\n",
      "ASL 300d 13 HS\n",
      "ASL 300d 13 MOV\n",
      "ASL 300d 13 LOC\n",
      "ASL 300d 14 ENTIRE\n",
      "ASL 300d 14 HS\n",
      "ASL 300d 14 MOV\n",
      "ASL 300d 14 LOC\n",
      "ASL 300d 15 ENTIRE\n",
      "ASL 300d 15 HS\n",
      "ASL 300d 15 MOV\n",
      "ASL 300d 15 LOC\n",
      "ASL 300d 16 ENTIRE\n",
      "ASL 300d 16 HS\n",
      "ASL 300d 16 MOV\n",
      "ASL 300d 16 LOC\n",
      "ASL 300d 17 ENTIRE\n",
      "ASL 300d 17 HS\n",
      "ASL 300d 17 MOV\n",
      "ASL 300d 17 LOC\n",
      "ASL 300d 18 ENTIRE\n",
      "ASL 300d 18 HS\n",
      "ASL 300d 18 MOV\n",
      "ASL 300d 18 LOC\n",
      "ASL 300d 19 ENTIRE\n",
      "ASL 300d 19 HS\n",
      "ASL 300d 19 MOV\n",
      "ASL 300d 19 LOC\n",
      "ASL 300d 20 ENTIRE\n",
      "ASL 300d 20 HS\n",
      "ASL 300d 20 MOV\n",
      "ASL 300d 20 LOC\n",
      "ASL 300d 21 ENTIRE\n",
      "ASL 300d 21 HS\n",
      "ASL 300d 21 MOV\n",
      "ASL 300d 21 LOC\n",
      "ASL 300d 22 ENTIRE\n",
      "ASL 300d 22 HS\n",
      "ASL 300d 22 MOV\n",
      "ASL 300d 22 LOC\n",
      "ASL 300d 23 ENTIRE\n",
      "ASL 300d 23 HS\n",
      "ASL 300d 23 MOV\n",
      "ASL 300d 23 LOC\n",
      "ASL 300d 24 ENTIRE\n",
      "ASL 300d 24 HS\n",
      "ASL 300d 24 MOV\n",
      "ASL 300d 24 LOC\n",
      "ASL 300d 25 ENTIRE\n",
      "ASL 300d 25 HS\n",
      "ASL 300d 25 MOV\n",
      "ASL 300d 25 LOC\n",
      "ASL 300d 26 ENTIRE\n",
      "ASL 300d 26 HS\n",
      "ASL 300d 26 MOV\n",
      "ASL 300d 26 LOC\n",
      "ASL 300d 27 ENTIRE\n",
      "ASL 300d 27 HS\n",
      "ASL 300d 27 MOV\n",
      "ASL 300d 27 LOC\n",
      "ASL 300d 28 ENTIRE\n",
      "ASL 300d 28 HS\n",
      "ASL 300d 28 MOV\n",
      "ASL 300d 28 LOC\n",
      "ASL 300d 29 ENTIRE\n",
      "ASL 300d 29 HS\n",
      "ASL 300d 29 MOV\n",
      "ASL 300d 29 LOC\n",
      "ASL 300d 30 ENTIRE\n",
      "ASL 300d 30 HS\n",
      "ASL 300d 30 MOV\n",
      "ASL 300d 30 LOC\n",
      "BSL 50d 2 ENTIRE\n",
      "BSL 50d 2 HS\n",
      "BSL 50d 2 MOV\n",
      "BSL 50d 2 LOC\n",
      "BSL 50d 3 ENTIRE\n",
      "BSL 50d 3 HS\n",
      "BSL 50d 3 MOV\n",
      "BSL 50d 3 LOC\n",
      "BSL 50d 4 ENTIRE\n",
      "BSL 50d 4 HS\n",
      "BSL 50d 4 MOV\n",
      "BSL 50d 4 LOC\n",
      "BSL 50d 5 ENTIRE\n",
      "BSL 50d 5 HS\n",
      "BSL 50d 5 MOV\n",
      "BSL 50d 5 LOC\n",
      "BSL 50d 6 ENTIRE\n",
      "BSL 50d 6 HS\n",
      "BSL 50d 6 MOV\n",
      "BSL 50d 6 LOC\n",
      "BSL 50d 7 ENTIRE\n",
      "BSL 50d 7 HS\n",
      "BSL 50d 7 MOV\n",
      "BSL 50d 7 LOC\n",
      "BSL 50d 8 ENTIRE\n",
      "BSL 50d 8 HS\n",
      "BSL 50d 8 MOV\n",
      "BSL 50d 8 LOC\n",
      "BSL 50d 9 ENTIRE\n",
      "BSL 50d 9 HS\n",
      "BSL 50d 9 MOV\n",
      "BSL 50d 9 LOC\n",
      "BSL 50d 10 ENTIRE\n",
      "BSL 50d 10 HS\n",
      "BSL 50d 10 MOV\n",
      "BSL 50d 10 LOC\n",
      "BSL 50d 11 ENTIRE\n",
      "BSL 50d 11 HS\n",
      "BSL 50d 11 MOV\n",
      "BSL 50d 11 LOC\n",
      "BSL 50d 12 ENTIRE\n",
      "BSL 50d 12 HS\n",
      "BSL 50d 12 MOV\n",
      "BSL 50d 12 LOC\n",
      "BSL 50d 13 ENTIRE\n",
      "BSL 50d 13 HS\n",
      "BSL 50d 13 MOV\n",
      "BSL 50d 13 LOC\n",
      "BSL 50d 14 ENTIRE\n",
      "BSL 50d 14 HS\n",
      "BSL 50d 14 MOV\n",
      "BSL 50d 14 LOC\n",
      "BSL 50d 15 ENTIRE\n",
      "BSL 50d 15 HS\n",
      "BSL 50d 15 MOV\n",
      "BSL 50d 15 LOC\n",
      "BSL 50d 16 ENTIRE\n",
      "BSL 50d 16 HS\n",
      "BSL 50d 16 MOV\n",
      "BSL 50d 16 LOC\n",
      "BSL 50d 17 ENTIRE\n",
      "BSL 50d 17 HS\n",
      "BSL 50d 17 MOV\n",
      "BSL 50d 17 LOC\n",
      "BSL 50d 18 ENTIRE\n",
      "BSL 50d 18 HS\n",
      "BSL 50d 18 MOV\n",
      "BSL 50d 18 LOC\n",
      "BSL 50d 19 ENTIRE\n",
      "BSL 50d 19 HS\n",
      "BSL 50d 19 MOV\n",
      "BSL 50d 19 LOC\n",
      "BSL 50d 20 ENTIRE\n",
      "BSL 50d 20 HS\n",
      "BSL 50d 20 MOV\n",
      "BSL 50d 20 LOC\n",
      "BSL 50d 21 ENTIRE\n",
      "BSL 50d 21 HS\n",
      "BSL 50d 21 MOV\n",
      "BSL 50d 21 LOC\n",
      "BSL 50d 22 ENTIRE\n",
      "BSL 50d 22 HS\n",
      "BSL 50d 22 MOV\n",
      "BSL 50d 22 LOC\n",
      "BSL 50d 23 ENTIRE\n",
      "BSL 50d 23 HS\n",
      "BSL 50d 23 MOV\n",
      "BSL 50d 23 LOC\n",
      "BSL 50d 24 ENTIRE\n",
      "BSL 50d 24 HS\n",
      "BSL 50d 24 MOV\n",
      "BSL 50d 24 LOC\n",
      "BSL 50d 25 ENTIRE\n",
      "BSL 50d 25 HS\n",
      "BSL 50d 25 MOV\n",
      "BSL 50d 25 LOC\n",
      "BSL 50d 26 ENTIRE\n",
      "BSL 50d 26 HS\n",
      "BSL 50d 26 MOV\n",
      "BSL 50d 26 LOC\n",
      "BSL 50d 27 ENTIRE\n",
      "BSL 50d 27 HS\n",
      "BSL 50d 27 MOV\n",
      "BSL 50d 27 LOC\n",
      "BSL 50d 28 ENTIRE\n",
      "BSL 50d 28 HS\n",
      "BSL 50d 28 MOV\n",
      "BSL 50d 28 LOC\n",
      "BSL 50d 29 ENTIRE\n",
      "BSL 50d 29 HS\n",
      "BSL 50d 29 MOV\n",
      "BSL 50d 29 LOC\n",
      "BSL 50d 30 ENTIRE\n",
      "BSL 50d 30 HS\n",
      "BSL 50d 30 MOV\n",
      "BSL 50d 30 LOC\n",
      "BSL 100d 2 ENTIRE\n",
      "BSL 100d 2 HS\n",
      "BSL 100d 2 MOV\n",
      "BSL 100d 2 LOC\n",
      "BSL 100d 3 ENTIRE\n",
      "BSL 100d 3 HS\n",
      "BSL 100d 3 MOV\n",
      "BSL 100d 3 LOC\n",
      "BSL 100d 4 ENTIRE\n",
      "BSL 100d 4 HS\n",
      "BSL 100d 4 MOV\n",
      "BSL 100d 4 LOC\n",
      "BSL 100d 5 ENTIRE\n",
      "BSL 100d 5 HS\n",
      "BSL 100d 5 MOV\n",
      "BSL 100d 5 LOC\n",
      "BSL 100d 6 ENTIRE\n",
      "BSL 100d 6 HS\n",
      "BSL 100d 6 MOV\n",
      "BSL 100d 6 LOC\n",
      "BSL 100d 7 ENTIRE\n",
      "BSL 100d 7 HS\n",
      "BSL 100d 7 MOV\n",
      "BSL 100d 7 LOC\n",
      "BSL 100d 8 ENTIRE\n",
      "BSL 100d 8 HS\n",
      "BSL 100d 8 MOV\n",
      "BSL 100d 8 LOC\n",
      "BSL 100d 9 ENTIRE\n",
      "BSL 100d 9 HS\n",
      "BSL 100d 9 MOV\n",
      "BSL 100d 9 LOC\n",
      "BSL 100d 10 ENTIRE\n",
      "BSL 100d 10 HS\n",
      "BSL 100d 10 MOV\n",
      "BSL 100d 10 LOC\n",
      "BSL 100d 11 ENTIRE\n",
      "BSL 100d 11 HS\n",
      "BSL 100d 11 MOV\n",
      "BSL 100d 11 LOC\n",
      "BSL 100d 12 ENTIRE\n",
      "BSL 100d 12 HS\n",
      "BSL 100d 12 MOV\n",
      "BSL 100d 12 LOC\n",
      "BSL 100d 13 ENTIRE\n",
      "BSL 100d 13 HS\n",
      "BSL 100d 13 MOV\n",
      "BSL 100d 13 LOC\n",
      "BSL 100d 14 ENTIRE\n",
      "BSL 100d 14 HS\n",
      "BSL 100d 14 MOV\n",
      "BSL 100d 14 LOC\n",
      "BSL 100d 15 ENTIRE\n",
      "BSL 100d 15 HS\n",
      "BSL 100d 15 MOV\n",
      "BSL 100d 15 LOC\n",
      "BSL 100d 16 ENTIRE\n",
      "BSL 100d 16 HS\n",
      "BSL 100d 16 MOV\n",
      "BSL 100d 16 LOC\n",
      "BSL 100d 17 ENTIRE\n",
      "BSL 100d 17 HS\n",
      "BSL 100d 17 MOV\n",
      "BSL 100d 17 LOC\n",
      "BSL 100d 18 ENTIRE\n",
      "BSL 100d 18 HS\n",
      "BSL 100d 18 MOV\n",
      "BSL 100d 18 LOC\n",
      "BSL 100d 19 ENTIRE\n",
      "BSL 100d 19 HS\n",
      "BSL 100d 19 MOV\n",
      "BSL 100d 19 LOC\n",
      "BSL 100d 20 ENTIRE\n",
      "BSL 100d 20 HS\n",
      "BSL 100d 20 MOV\n",
      "BSL 100d 20 LOC\n",
      "BSL 100d 21 ENTIRE\n",
      "BSL 100d 21 HS\n",
      "BSL 100d 21 MOV\n",
      "BSL 100d 21 LOC\n",
      "BSL 100d 22 ENTIRE\n",
      "BSL 100d 22 HS\n",
      "BSL 100d 22 MOV\n",
      "BSL 100d 22 LOC\n",
      "BSL 100d 23 ENTIRE\n",
      "BSL 100d 23 HS\n",
      "BSL 100d 23 MOV\n",
      "BSL 100d 23 LOC\n",
      "BSL 100d 24 ENTIRE\n",
      "BSL 100d 24 HS\n",
      "BSL 100d 24 MOV\n",
      "BSL 100d 24 LOC\n",
      "BSL 100d 25 ENTIRE\n",
      "BSL 100d 25 HS\n",
      "BSL 100d 25 MOV\n",
      "BSL 100d 25 LOC\n",
      "BSL 100d 26 ENTIRE\n",
      "BSL 100d 26 HS\n",
      "BSL 100d 26 MOV\n",
      "BSL 100d 26 LOC\n",
      "BSL 100d 27 ENTIRE\n",
      "BSL 100d 27 HS\n",
      "BSL 100d 27 MOV\n",
      "BSL 100d 27 LOC\n",
      "BSL 100d 28 ENTIRE\n",
      "BSL 100d 28 HS\n",
      "BSL 100d 28 MOV\n",
      "BSL 100d 28 LOC\n",
      "BSL 100d 29 ENTIRE\n",
      "BSL 100d 29 HS\n",
      "BSL 100d 29 MOV\n",
      "BSL 100d 29 LOC\n",
      "BSL 100d 30 ENTIRE\n",
      "BSL 100d 30 HS\n",
      "BSL 100d 30 MOV\n",
      "BSL 100d 30 LOC\n",
      "BSL 200d 2 ENTIRE\n",
      "BSL 200d 2 HS\n",
      "BSL 200d 2 MOV\n",
      "BSL 200d 2 LOC\n",
      "BSL 200d 3 ENTIRE\n",
      "BSL 200d 3 HS\n",
      "BSL 200d 3 MOV\n",
      "BSL 200d 3 LOC\n",
      "BSL 200d 4 ENTIRE\n",
      "BSL 200d 4 HS\n",
      "BSL 200d 4 MOV\n",
      "BSL 200d 4 LOC\n",
      "BSL 200d 5 ENTIRE\n",
      "BSL 200d 5 HS\n",
      "BSL 200d 5 MOV\n",
      "BSL 200d 5 LOC\n",
      "BSL 200d 6 ENTIRE\n",
      "BSL 200d 6 HS\n",
      "BSL 200d 6 MOV\n",
      "BSL 200d 6 LOC\n",
      "BSL 200d 7 ENTIRE\n",
      "BSL 200d 7 HS\n",
      "BSL 200d 7 MOV\n",
      "BSL 200d 7 LOC\n",
      "BSL 200d 8 ENTIRE\n",
      "BSL 200d 8 HS\n",
      "BSL 200d 8 MOV\n",
      "BSL 200d 8 LOC\n",
      "BSL 200d 9 ENTIRE\n",
      "BSL 200d 9 HS\n",
      "BSL 200d 9 MOV\n",
      "BSL 200d 9 LOC\n",
      "BSL 200d 10 ENTIRE\n",
      "BSL 200d 10 HS\n",
      "BSL 200d 10 MOV\n",
      "BSL 200d 10 LOC\n",
      "BSL 200d 11 ENTIRE\n",
      "BSL 200d 11 HS\n",
      "BSL 200d 11 MOV\n",
      "BSL 200d 11 LOC\n",
      "BSL 200d 12 ENTIRE\n",
      "BSL 200d 12 HS\n",
      "BSL 200d 12 MOV\n",
      "BSL 200d 12 LOC\n",
      "BSL 200d 13 ENTIRE\n",
      "BSL 200d 13 HS\n",
      "BSL 200d 13 MOV\n",
      "BSL 200d 13 LOC\n",
      "BSL 200d 14 ENTIRE\n",
      "BSL 200d 14 HS\n",
      "BSL 200d 14 MOV\n",
      "BSL 200d 14 LOC\n",
      "BSL 200d 15 ENTIRE\n",
      "BSL 200d 15 HS\n",
      "BSL 200d 15 MOV\n",
      "BSL 200d 15 LOC\n",
      "BSL 200d 16 ENTIRE\n",
      "BSL 200d 16 HS\n",
      "BSL 200d 16 MOV\n",
      "BSL 200d 16 LOC\n",
      "BSL 200d 17 ENTIRE\n",
      "BSL 200d 17 HS\n",
      "BSL 200d 17 MOV\n",
      "BSL 200d 17 LOC\n",
      "BSL 200d 18 ENTIRE\n",
      "BSL 200d 18 HS\n",
      "BSL 200d 18 MOV\n",
      "BSL 200d 18 LOC\n",
      "BSL 200d 19 ENTIRE\n",
      "BSL 200d 19 HS\n",
      "BSL 200d 19 MOV\n",
      "BSL 200d 19 LOC\n",
      "BSL 200d 20 ENTIRE\n",
      "BSL 200d 20 HS\n",
      "BSL 200d 20 MOV\n",
      "BSL 200d 20 LOC\n",
      "BSL 200d 21 ENTIRE\n",
      "BSL 200d 21 HS\n",
      "BSL 200d 21 MOV\n",
      "BSL 200d 21 LOC\n",
      "BSL 200d 22 ENTIRE\n",
      "BSL 200d 22 HS\n",
      "BSL 200d 22 MOV\n",
      "BSL 200d 22 LOC\n",
      "BSL 200d 23 ENTIRE\n",
      "BSL 200d 23 HS\n",
      "BSL 200d 23 MOV\n",
      "BSL 200d 23 LOC\n",
      "BSL 200d 24 ENTIRE\n",
      "BSL 200d 24 HS\n",
      "BSL 200d 24 MOV\n",
      "BSL 200d 24 LOC\n",
      "BSL 200d 25 ENTIRE\n",
      "BSL 200d 25 HS\n",
      "BSL 200d 25 MOV\n",
      "BSL 200d 25 LOC\n",
      "BSL 200d 26 ENTIRE\n",
      "BSL 200d 26 HS\n",
      "BSL 200d 26 MOV\n",
      "BSL 200d 26 LOC\n",
      "BSL 200d 27 ENTIRE\n",
      "BSL 200d 27 HS\n",
      "BSL 200d 27 MOV\n",
      "BSL 200d 27 LOC\n",
      "BSL 200d 28 ENTIRE\n",
      "BSL 200d 28 HS\n",
      "BSL 200d 28 MOV\n",
      "BSL 200d 28 LOC\n",
      "BSL 200d 29 ENTIRE\n",
      "BSL 200d 29 HS\n",
      "BSL 200d 29 MOV\n",
      "BSL 200d 29 LOC\n",
      "BSL 200d 30 ENTIRE\n",
      "BSL 200d 30 HS\n",
      "BSL 200d 30 MOV\n",
      "BSL 200d 30 LOC\n",
      "BSL 300d 2 ENTIRE\n",
      "BSL 300d 2 HS\n",
      "BSL 300d 2 MOV\n",
      "BSL 300d 2 LOC\n",
      "BSL 300d 3 ENTIRE\n",
      "BSL 300d 3 HS\n",
      "BSL 300d 3 MOV\n",
      "BSL 300d 3 LOC\n",
      "BSL 300d 4 ENTIRE\n",
      "BSL 300d 4 HS\n",
      "BSL 300d 4 MOV\n",
      "BSL 300d 4 LOC\n",
      "BSL 300d 5 ENTIRE\n",
      "BSL 300d 5 HS\n",
      "BSL 300d 5 MOV\n",
      "BSL 300d 5 LOC\n",
      "BSL 300d 6 ENTIRE\n",
      "BSL 300d 6 HS\n",
      "BSL 300d 6 MOV\n",
      "BSL 300d 6 LOC\n",
      "BSL 300d 7 ENTIRE\n",
      "BSL 300d 7 HS\n",
      "BSL 300d 7 MOV\n",
      "BSL 300d 7 LOC\n",
      "BSL 300d 8 ENTIRE\n",
      "BSL 300d 8 HS\n",
      "BSL 300d 8 MOV\n",
      "BSL 300d 8 LOC\n",
      "BSL 300d 9 ENTIRE\n",
      "BSL 300d 9 HS\n",
      "BSL 300d 9 MOV\n",
      "BSL 300d 9 LOC\n",
      "BSL 300d 10 ENTIRE\n",
      "BSL 300d 10 HS\n",
      "BSL 300d 10 MOV\n",
      "BSL 300d 10 LOC\n",
      "BSL 300d 11 ENTIRE\n",
      "BSL 300d 11 HS\n",
      "BSL 300d 11 MOV\n",
      "BSL 300d 11 LOC\n",
      "BSL 300d 12 ENTIRE\n",
      "BSL 300d 12 HS\n",
      "BSL 300d 12 MOV\n",
      "BSL 300d 12 LOC\n",
      "BSL 300d 13 ENTIRE\n",
      "BSL 300d 13 HS\n",
      "BSL 300d 13 MOV\n",
      "BSL 300d 13 LOC\n",
      "BSL 300d 14 ENTIRE\n",
      "BSL 300d 14 HS\n",
      "BSL 300d 14 MOV\n",
      "BSL 300d 14 LOC\n",
      "BSL 300d 15 ENTIRE\n",
      "BSL 300d 15 HS\n",
      "BSL 300d 15 MOV\n",
      "BSL 300d 15 LOC\n",
      "BSL 300d 16 ENTIRE\n",
      "BSL 300d 16 HS\n",
      "BSL 300d 16 MOV\n",
      "BSL 300d 16 LOC\n",
      "BSL 300d 17 ENTIRE\n",
      "BSL 300d 17 HS\n",
      "BSL 300d 17 MOV\n",
      "BSL 300d 17 LOC\n",
      "BSL 300d 18 ENTIRE\n",
      "BSL 300d 18 HS\n",
      "BSL 300d 18 MOV\n",
      "BSL 300d 18 LOC\n",
      "BSL 300d 19 ENTIRE\n",
      "BSL 300d 19 HS\n",
      "BSL 300d 19 MOV\n",
      "BSL 300d 19 LOC\n",
      "BSL 300d 20 ENTIRE\n",
      "BSL 300d 20 HS\n",
      "BSL 300d 20 MOV\n",
      "BSL 300d 20 LOC\n",
      "BSL 300d 21 ENTIRE\n",
      "BSL 300d 21 HS\n",
      "BSL 300d 21 MOV\n",
      "BSL 300d 21 LOC\n",
      "BSL 300d 22 ENTIRE\n",
      "BSL 300d 22 HS\n",
      "BSL 300d 22 MOV\n",
      "BSL 300d 22 LOC\n",
      "BSL 300d 23 ENTIRE\n",
      "BSL 300d 23 HS\n",
      "BSL 300d 23 MOV\n",
      "BSL 300d 23 LOC\n",
      "BSL 300d 24 ENTIRE\n",
      "BSL 300d 24 HS\n",
      "BSL 300d 24 MOV\n",
      "BSL 300d 24 LOC\n",
      "BSL 300d 25 ENTIRE\n",
      "BSL 300d 25 HS\n",
      "BSL 300d 25 MOV\n",
      "BSL 300d 25 LOC\n",
      "BSL 300d 26 ENTIRE\n",
      "BSL 300d 26 HS\n",
      "BSL 300d 26 MOV\n",
      "BSL 300d 26 LOC\n",
      "BSL 300d 27 ENTIRE\n",
      "BSL 300d 27 HS\n",
      "BSL 300d 27 MOV\n",
      "BSL 300d 27 LOC\n",
      "BSL 300d 28 ENTIRE\n",
      "BSL 300d 28 HS\n",
      "BSL 300d 28 MOV\n",
      "BSL 300d 28 LOC\n",
      "BSL 300d 29 ENTIRE\n",
      "BSL 300d 29 HS\n",
      "BSL 300d 29 MOV\n",
      "BSL 300d 29 LOC\n",
      "BSL 300d 30 ENTIRE\n",
      "BSL 300d 30 HS\n",
      "BSL 300d 30 MOV\n",
      "BSL 300d 30 LOC\n"
     ]
    }
   ],
   "source": [
    "languages = [\"ASL\",\n",
    "             \"BSL\"\n",
    "            ]\n",
    "dims = [\"50d\", \n",
    "        \"100d\", \"200d\", \"300d\"\n",
    "       ]\n",
    "\n",
    "phonTypes = [\"ENTIRE\", \"HS\", \"MOV\", \"LOC\"]\n",
    "heights = [i for i in range(lowerBound, upperBound)]\n",
    "\n",
    "\n",
    "phon_masterPath = \"data/output/vectorizedPhonDFs_Stitched_DFs/\"\n",
    "sem_masterPath = \"data/output/SemSim_StitchedDFs/\"\n",
    "\n",
    "finalDF = pd.DataFrame()\n",
    "for (language, dim, heit) in product(languages, dims, heights):\n",
    "    semDF = pd.read_csv(sem_masterPath+language+\"_\"+dim+\".csv.gz\")\n",
    "    semDF[\"signPair\"] = semDF[\"s1\"] + \" + \" + semDF[\"s2\"]\n",
    "\n",
    "    df = allClusters[(allClusters.language == language) & (allClusters.dim == dim) & (allClusters.height==heit)].set_index(\"signPair\")\n",
    "    df = df.merge(semDF, on='signPair')\n",
    "    df = df.drop([\"s1\", \"s2\", \"semSim\"], axis=1)\n",
    "    \n",
    "    for phonType in phonTypes:\n",
    "        print(language, dim, heit, phonType)\n",
    "        phonDF = pd.read_csv(phon_masterPath+language+\"_\"+phonType+\".csv.gz\")\n",
    "        phonDF[\"signPair\"] = phonDF[\"s1\"] + \" + \" + phonDF[\"s2\"]\n",
    "        \n",
    "        df = df.merge(phonDF, on='signPair')\n",
    "        df = df.drop([\"s1\", \"s2\", phonType+\"_sim\"], axis=1)\n",
    "        \n",
    "    finalDF = finalDF.append(df, ignore_index=True)\n",
    "    \n",
    "finalDF.to_csv(\"results/clustering/allClusters_withValues.csv.gz\", compression = \"gzip\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d2d678-b180-4225-9af6-e0d258126c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>dim</th>\n",
       "      <th>prune_height</th>\n",
       "      <th>phonType</th>\n",
       "      <th>pearsonR_with_semSim</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASL</td>\n",
       "      <td>50d</td>\n",
       "      <td>2</td>\n",
       "      <td>ENTIRE</td>\n",
       "      <td>0.324576</td>\n",
       "      <td>0.014663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASL</td>\n",
       "      <td>50d</td>\n",
       "      <td>2</td>\n",
       "      <td>HS</td>\n",
       "      <td>0.1844</td>\n",
       "      <td>0.173673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASL</td>\n",
       "      <td>50d</td>\n",
       "      <td>2</td>\n",
       "      <td>MOV</td>\n",
       "      <td>0.253123</td>\n",
       "      <td>0.059802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASL</td>\n",
       "      <td>50d</td>\n",
       "      <td>2</td>\n",
       "      <td>LOC</td>\n",
       "      <td>0.303602</td>\n",
       "      <td>0.022925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASL</td>\n",
       "      <td>50d</td>\n",
       "      <td>3</td>\n",
       "      <td>ENTIRE</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.00554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>BSL</td>\n",
       "      <td>300d</td>\n",
       "      <td>29</td>\n",
       "      <td>LOC</td>\n",
       "      <td>0.578992</td>\n",
       "      <td>0.13261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>BSL</td>\n",
       "      <td>300d</td>\n",
       "      <td>30</td>\n",
       "      <td>ENTIRE</td>\n",
       "      <td>0.401742</td>\n",
       "      <td>0.37166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>BSL</td>\n",
       "      <td>300d</td>\n",
       "      <td>30</td>\n",
       "      <td>HS</td>\n",
       "      <td>0.51672</td>\n",
       "      <td>0.235044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>BSL</td>\n",
       "      <td>300d</td>\n",
       "      <td>30</td>\n",
       "      <td>MOV</td>\n",
       "      <td>-0.217796</td>\n",
       "      <td>0.638965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>BSL</td>\n",
       "      <td>300d</td>\n",
       "      <td>30</td>\n",
       "      <td>LOC</td>\n",
       "      <td>0.506421</td>\n",
       "      <td>0.246135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    language   dim prune_height phonType pearsonR_with_semSim   p-value\n",
       "0        ASL   50d            2   ENTIRE             0.324576  0.014663\n",
       "1        ASL   50d            2       HS               0.1844  0.173673\n",
       "2        ASL   50d            2      MOV             0.253123  0.059802\n",
       "3        ASL   50d            2      LOC             0.303602  0.022925\n",
       "4        ASL   50d            3   ENTIRE             0.174608   0.00554\n",
       "..       ...   ...          ...      ...                  ...       ...\n",
       "923      BSL  300d           29      LOC             0.578992   0.13261\n",
       "924      BSL  300d           30   ENTIRE             0.401742   0.37166\n",
       "925      BSL  300d           30       HS              0.51672  0.235044\n",
       "926      BSL  300d           30      MOV            -0.217796  0.638965\n",
       "927      BSL  300d           30      LOC             0.506421  0.246135\n",
       "\n",
       "[928 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = (finalDF.groupby([\"language\", \"dim\", \"height\", \"clusterID\"]).sum()/finalDF.groupby([\"language\", \"dim\", \"height\", \"clusterID\"]).count()).reset_index()\n",
    "\n",
    "languages = [\"ASL\",\n",
    "             \"BSL\"\n",
    "            ]\n",
    "\n",
    "dims = [\"50d\", \n",
    "        \"100d\", \"200d\", \"300d\"\n",
    "       ]\n",
    "\n",
    "phonTypes = [\"ENTIRE\", \"HS\", \"MOV\", \"LOC\"]\n",
    "\n",
    "heights = [i for i in range(lowerBound, upperBound)]\n",
    "\n",
    "corrs = pd.DataFrame(columns=[\"language\", \"dim\", \"prune_height\", \"phonType\", \"pearsonR_with_semSim\", \"p-value\"], index=range(len(languages)*len(dims)*len(heights)*len(phonTypes)))\n",
    "i = 0\n",
    "for (language, dim, heit) in product(languages, dims, heights):\n",
    "    df = summed[(summed.language == language) & (summed.dim == dim) & (summed.height==heit)]\n",
    "    \n",
    "    for phonType in phonTypes:\n",
    "        r, p = pearsonr(df[\"sem_cosineSim\"], df[phonType+\"_cosineSim\"])\n",
    "        \n",
    "        corrs.iloc[i][\"language\"] = language\n",
    "        corrs.iloc[i][\"dim\"] = dim\n",
    "        corrs.iloc[i][\"prune_height\"] = heit\n",
    "        corrs.iloc[i][\"phonType\"] = phonType\n",
    "        corrs.iloc[i][\"pearsonR_with_semSim\"] = r\n",
    "        corrs.iloc[i][\"p-value\"] = p\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "corrs.to_csv(\"results/clustering/clustering_corr_coefs.csv\", index=False)\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f74840-3323-4b0a-bda6-c5f02846b261",
   "metadata": {},
   "source": [
    "# ***IGNORE AFTER HERE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4eb960-e8a0-4a61-9cee-130552538c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allClusters.groupby([\"language\", \"dim\", \"height\"])[\"signPair\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987ea75-dc66-477c-9684-342d01760206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotOutputPath = \"plots/heights_elbow/\"\n",
    "\n",
    "# if not os.path.exists(plotOutputPath):\n",
    "#     os.makedirs(plotOutputPath)\n",
    "    \n",
    "# y_ticks = np.arange(0, 2000, 400)\n",
    "\n",
    "\n",
    "\n",
    "# for p in semPaths[:limit]:\n",
    "#     plt.figure(figsize=(12,12))\n",
    "#     print(p, \"\\n\")\n",
    "    \n",
    "#     language = p.split(\"/\")[-1].split(\"_\")[0]\n",
    "#     dim = p.split(\"/\")[-1].split(\"_\")[2]\n",
    "    \n",
    "#     heights = []\n",
    "#     for heit in range(100):\n",
    "#         clusters = cluster_glove(p, height=heit)\n",
    "#         clustersN = len(list(set([x[0] for x in clusters])))\n",
    "#         heights += [(heit, clustersN)]\n",
    "        \n",
    "#     plt.scatter([x[0] for x in heights], [x[1] for x in heights])\n",
    "#     _=plt.yticks(y_ticks)\n",
    "#     _=plt.axes().set_ylim(-100, 2000)\n",
    "#     _=plt.axes().set_xlim(-5,100)\n",
    "#     plt.savefig(plotOutputPath+language+\"_\"+dim+\".png\", dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505185f-e1f0-4fd2-b8ef-293d89e69b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22f565-d43f-4134-af08-e802643cbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd, numpy as np, os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from scipy.cluster.hierarchy import dendrogram\n",
    "# from scipy.cluster import hierarchy\n",
    "\n",
    "# distance_Tresholds = [\n",
    "#     6,\n",
    "# #                         7,\n",
    "# #                       8,9,10,\n",
    "# #                       11, 12,\n",
    "#     13, 14, 15\n",
    "    \n",
    "#                      ]\n",
    "\n",
    "# inputPath = semPaths\n",
    "# outputPath = \"../../04.Analyses/hierarchicalClustering/gloVe_VSMs_ClusteredHierarchical/\"\n",
    "# gloves = [x for x in os.listdir(inputPath) if not x.startswith(\".\")]\n",
    "\n",
    "# method = \"ward\"\n",
    "# distanceMethod = \"euclidean\"\n",
    "\n",
    "# Limit = None\n",
    "# for glove in gloves[:Limit]:\n",
    "#     datasetName = glove\n",
    "#     print(\"Now working on \", glove)\n",
    "#     data = pd.read_csv(inputPath+datasetName).set_index(\"label\")[:]\n",
    "#     print(\"length of data:\", len(data))\n",
    "#     data.head()\n",
    "#     dataOutput = data.reset_index()\n",
    "#     X = data\n",
    "#     signs = [x for x in data.index]\n",
    "#     # print(signs)\n",
    "\n",
    "#     for threshold in distance_Tresholds:\n",
    "#         model = AgglomerativeClustering(linkage= method,\n",
    "#                                         affinity= distanceMethod,\n",
    "#                                         distance_threshold=threshold,\n",
    "#                                         n_clusters=None,\n",
    "#                                         compute_distances=True\n",
    "#                                        )\n",
    "#         model_fit = model.fit(X)\n",
    "#         clusters = [\"C\"+str(c) for c in list(model.fit_predict(X))]\n",
    "#         dataOutput[\"clusters\"] = clusters\n",
    "# #             dataOutput[\"labels\"] = signs\n",
    "\n",
    "#         dataOutput.to_csv(outputPath+glove[:-4]+\"_height\"+str(threshold)+\".csv\", index=False)\n",
    "\n",
    "\n",
    "#         print(\"N of clusters: \", model_fit.n_clusters_)\n",
    "\n",
    "\n",
    "# print(\"this cell executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e766a-c827-4811-9d7c-4df99efa51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"data/output/vectorizedPhonDFs/ASL_HS_vectorizedDF.csv.gz\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2ec79-4d8e-4a95-8938-85de4da050e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
