{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44af9ff6-021c-40d0-b5d6-9f866c2f9578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cell executed.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os, pandas as pd, numpy as np, pprint as pp, matplotlib.pyplot as plt, json\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "from clustering.clustering_funcs import cluster_glove\n",
    "from phonology.funcs import vectorize_phonology, find_phonology_cosine_similarity_perPhonType, stitch_parts\n",
    "from preprocess.funcs import remove_phonology_duplicate_videos, clear_phonology_df, clear_semantics_df\n",
    "from semantics.funcs import find_semantics_cosine_similarity_pairwise\n",
    "from utility.util_funcs import pandas_pair_signs_alphabetically\n",
    "from itertools import combinations\n",
    "\n",
    "from itertools import product\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c5b65-408b-4ed6-8fc4-84589e661e80",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing\n",
    "\n",
    "In this part, we transform the datasets that we have to a format that allows comparisons:\n",
    "\n",
    "- We use pretrained English GloVe vectors (trained on the Wikipedia corpus) to extract semantic similarity between signs\n",
    "- For phonological similarity, we use annotated signs from the Gallaudet Dictionary for ASL, and the SignBank for BSL\n",
    "- The method that we follow does not allow a *direct* comparison between ASL and BSL. We use the available phonological annotations as the basis of our semantic space, which have different entries across the two languages; therefore, the semantic space, while they do share ~500 signs, are not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e06ac-a124-4b33-9ab3-df51ac9af743",
   "metadata": {},
   "source": [
    "## 1.1. Filtering the data\n",
    "\n",
    "In this section, we identify the ASL and the BSL signs that we will be working with.\n",
    "\n",
    "Each sign:\n",
    "- Has to have a phonological transcription\n",
    "- Has to have a semantic vector representation in the GloVe vectors\n",
    "- Must not have duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2184a422-ed4f-4fd7-a8cb-e2b645d42c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw/phonologyData/ASL_Phonology_Entire.xlsx\n",
      "data/raw/phonologyData/BSL_Phonology_Entire.xlsx\n",
      "\n",
      "\n",
      "Cell executed.\n"
     ]
    }
   ],
   "source": [
    "phonRoot_raw = \"data/raw/phonologyData/\"\n",
    "phon_paths_raw = [phonRoot_raw+p for p in os.listdir(phonRoot_raw) if (not p.startswith(\".\")) and (p.endswith(\"xlsx\"))]\n",
    "\n",
    "for p in phon_paths_raw:\n",
    "    print(p)\n",
    "    remove_phonology_duplicate_videos(p)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8cd84f-bf32-4880-8736-a793af038684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cell executed.\n"
     ]
    }
   ],
   "source": [
    "phonRoot_unique = \"data/transforming/phonologyData/unique_signs/\"\n",
    "phon_paths_uniqueSigns = [phonRoot_unique+p for p in os.listdir(phonRoot_unique) if (not p.startswith(\".\")) and (p.endswith(\"gz\"))]\n",
    "\n",
    "gloveRoot = \"../../../Downloads/glove/\" #replace with path to Glove txt files\n",
    "glovePaths = [gloveRoot+g for g in os.listdir(gloveRoot) if not g.startswith(\".\")]\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b49629-9e07-4431-a252-76bd35e8c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/transforming/phonologyData/unique_signs/BSL_unique.csv.gz\n",
      "1.97 s ± 22.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "data/transforming/phonologyData/unique_signs/ASL_unique.csv.gz\n",
      "2.03 s ± 77.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "\n",
      "Cell executed.\n"
     ]
    }
   ],
   "source": [
    "for p in phon_paths_uniqueSigns:\n",
    "    print(p)\n",
    "    %timeit clear_phonology_df(p)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95307d-6e70-4d2c-babf-371fed0bc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/transforming/phonologyData/unique_signs/BSL_unique.csv.gz ../../../Downloads/glove/glove.6B.300d.txt\n",
      "14 s ± 126 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "data/transforming/phonologyData/unique_signs/BSL_unique.csv.gz ../../../Downloads/glove/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "#Uncomment below if you want to run - Takes about 5 minutes\n",
    "for (p, g) in product(phon_paths_uniqueSigns, glovePaths):\n",
    "    print(p, g)\n",
    "    %timeit clear_semantics_df(p, g)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1bbeb-e313-4145-aa8d-2b484b304608",
   "metadata": {},
   "source": [
    "## 1.2. Transforming and Vectorizing the Phonology Data\n",
    "\n",
    "In this section, we transform the phonology dataframes of ASL and BSL with the following goals in mind:\n",
    "\n",
    "- All signs are lower-cased.\n",
    "- Duplicate signs are removed. Only the first occurrence of a sign is kept. This is a necessary step to avoid skewing the data. For instance, the BSL phonology dataframe has more than 10 entries for the sign MAUVE. We only keep the first occurrence of MAUVE in the order of the rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03361c01-e07f-4956-9e75-bb8e7185cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonRoot_clean = \"data/output/phonologyData/\"\n",
    "phonPaths_clean = [phonRoot_clean+p for p in os.listdir(phonRoot_clean) if not p.startswith(\".\")]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about a couple minutes\n",
    "for p in phonPaths_clean:\n",
    "    print(p)\n",
    "    vectorize_phonology(p)\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ec9e5-477b-47ed-84e2-bb5117d69853",
   "metadata": {},
   "source": [
    "## 2. Finding Phonological Similarity (Pairwise Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5400f89-4ef6-4b33-9661-0af79dccf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FINDING PHONOLOGICAL SIMILARITY FROM VECTORIZED DFs\"\"\"\n",
    "phonRoot_vectorized = \"data/output/vectorizedPhonDFs/\"\n",
    "phonPaths_vectorized = [phonRoot_vectorized+p for p in os.listdir(phonRoot_vectorized) if not p.startswith(\".\")]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about over hour\n",
    "for p in phonPaths_vectorized:\n",
    "    print(p)\n",
    "#     find_phonology_cosine_similarity_perPhonType(p)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562db36-7067-4a7a-b292-bbe21d9606c0",
   "metadata": {},
   "source": [
    "## 3. Finding Semantic Similarity (Pairwise Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2384b4-cdaf-42e7-bf8f-e922004951a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "semPath_root = \"data/output/semanticsData/\"\n",
    "semPaths = [semPath_root+p for p in os.listdir(semPath_root) if not p.startswith(\".\")]\n",
    "\n",
    "#Uncomment below if you want to run - Takes over an hour\n",
    "for p in semPaths:\n",
    "    print(p)\n",
    "#     find_semantics_cosine_similarity_pairwise(p)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497e6cf-85d9-4414-87de-c09cbebcfc91",
   "metadata": {},
   "source": [
    "## 4. Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3189d-0e49-425e-84bf-a1872d3065f3",
   "metadata": {},
   "source": [
    "## 4.1. Pairwise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39142d-2193-43db-8438-b20abf5ebdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STITCHING TOGETHER PHONOLOGICAL SIMILARITY CSVs\"\"\"\n",
    "\n",
    "masterPath = \"data/output/vectorizedPhonDFs_Stitched_DFs/\"\n",
    "if not os.path.exists(masterPath):\n",
    "    os.makedirs(masterPath)\n",
    "    \n",
    "languages = [\"ASL\", \"BSL\"]\n",
    "phonTypes = [\"ENTIRE\", \"LOC\", \"MOV\", \"HS\"]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about 5 minutes\n",
    "for language, phonType in product(languages, phonTypes):\n",
    "    print(language, phonType)\n",
    "    path = \"data/output/vectorized_PhonSim/\"+language+\"/\"+phonType+\"/\"\n",
    "#     df = stitch_parts(path)\n",
    "#     df.to_csv(masterPath+language+\"_\"+phonType+\".csv.gz\", compression=\"gzip\",index=False)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a2836-15b8-440f-bc2a-75e424ac47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STITCHING TOGETHER SEMANTIC SIMILARITY CSVs\"\"\"\n",
    "\n",
    "masterPath = \"data/output/SemSim_StitchedDFs/\"\n",
    "if not os.path.exists(masterPath):\n",
    "    os.makedirs(masterPath)\n",
    "    \n",
    "languages = [\"ASL\", \"BSL\"]\n",
    "dims = [\"50d\", \"100d\", \"200d\", \"300d\"]\n",
    "\n",
    "#Uncomment below if you want to run - Takes about 5 minutes\n",
    "for language, dim in product(languages, dims):\n",
    "    print(language, dim)\n",
    "    path = \"data/output/SemSim/\"+language+\"/\"+dim+\"/\"\n",
    "#     df = stitch_parts(path)\n",
    "#     df.to_csv(masterPath+language+\"_\"+dim+\".csv.gz\", compression=\"gzip\",index=False)\n",
    "    \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84fdb4-3df2-4fa9-b653-858e23795ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PAIRWISE CORRELATIONS ANALYSIS\"\"\"\n",
    "\n",
    "masterPhonPath = \"data/output/vectorizedPhonDFs_Stitched_DFs/\"\n",
    "masterSemPath = \"data/output/SemSim_StitchedDFs/\"\n",
    "semPaths = [masterSemPath + p for p in os.listdir(masterSemPath) if not p.startswith(\".\")]\n",
    "phonPaths = [masterPhonPath + p for p in os.listdir(masterPhonPath) if not p.startswith(\".\")]\n",
    "\n",
    "#Creating a dataframe to store pairwise calculations\n",
    "df_pairwise_calculations = pd.DataFrame(columns=[\"phonType\", \"semDimension\", \"language\", \"pearson_r\", \"p-value\"], index=range(32))\n",
    "\n",
    "\n",
    "df_Limit = None\n",
    "i = -1\n",
    "for s, p in list(product(semPaths, phonPaths))[:df_Limit]:\n",
    "    language_sem = s.split(\"/\")[-1].split(\"_\")[0]\n",
    "    language_phon = p.split(\"/\")[-1].split(\"_\")[0]\n",
    "    \n",
    "    if language_sem == language_phon:\n",
    "        i += 1\n",
    "        \n",
    "        language = language_sem\n",
    "        dim = s.split(\"/\")[-1].split(\"_\")[1].split(\".\")[0]\n",
    "        phonType = p.split(\"/\")[-1].split(\"_\")[1].split(\".\")[0]\n",
    "        \n",
    "        print(i, language, dim, phonType)\n",
    "        \n",
    "        df_pairwise_calculations.iloc[i][\"language\"] = language\n",
    "        df_pairwise_calculations.iloc[i][\"phonType\"] = phonType\n",
    "        df_pairwise_calculations.iloc[i][\"semDimension\"] = dim\n",
    "        \n",
    "        #Loading CSVs\n",
    "        phonDF = pd.read_csv(p)\n",
    "        semDF = pd.read_csv(s)\n",
    "        \n",
    "        phonDF[\"paired\"] = phonDF.apply(lambda x: pandas_pair_signs_alphabetically(x), axis=1)\n",
    "        semDF[\"paired\"] = semDF.apply(lambda x: pandas_pair_signs_alphabetically(x), axis=1)\n",
    "        \n",
    "        phonDF = phonDF.drop([\"s1\", \"s2\"], axis=1).set_index(\"paired\").sort_index()\n",
    "        semDF = semDF.drop([\"s1\", \"s2\"], axis=1).set_index(\"paired\").sort_index()\n",
    "        \n",
    "        \n",
    "        if all(phonDF.index == semDF.index):\n",
    "            print(\"All indices match.\\n\\n\")\n",
    "        else:\n",
    "            print(\"INDICES DO NOT MATCH.\\n\\n\")\n",
    "            \n",
    "        #Calculate correlations\n",
    "        df_pairwise_calculations.iloc[i][\"pearson_r\"], df_pairwise_calculations.iloc[i][\"p-value\"] = pearsonr(phonDF[phonType+\"_cosineSim\"], semDF[\"sem_cosineSim\"])\n",
    "        \n",
    "        \"\"\"ADD VISUALIZATIONS HERE\"\"\"\n",
    "        \n",
    "        del phonDF\n",
    "        del semDF\n",
    "        \n",
    "df_pairwise_calculations.to_csv(\"results/pairwise/pairwise_results.csv\", index=False)\n",
    "df_pairwise_calculations\n",
    "\n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c886d-88d2-4d90-a5f4-a72fe1ced2b4",
   "metadata": {},
   "source": [
    "### Results of the Pairwise Analysis:\n",
    "\n",
    "There is no apparent linear relationship between phonological similarity (as measured by the additive inverse of cosine distance between two signs in space that are vectorized) and semantic similarity (measured using the same cosine method as phonological similarity -- except we use GloVe vectors pretrained on the Wikipedia corpus).\n",
    "\n",
    "Phonology is arbitrary when the lexicon of an SL is taken as a whole.\n",
    "\n",
    "This brings us to our next analysis: Hierarchical Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec65882-d7b1-4a35-831a-44ebef30cae4",
   "metadata": {},
   "source": [
    "## 4.2. Hierarchical Clustering Analysis\n",
    "\n",
    "In this section, we raise the question that if there is no linear relationship in the phonology and semantics of pairs of signs in a semantically ***unorganized*** lexicon of an SL, can we find relationships betweeen pairs of signs within clusters of semantically related signs? \n",
    "\n",
    "1. We first cluster signs in a semantic vector space using agglomerative hierarchical clustering\n",
    "2. We then look for pairwise relations between pairs of signs within individual clusters.\n",
    "3. This dramatically reduces the number of sign pairs that we study, as the pairing process does not cross cluster boundaries in a given semantic vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0584b-dd89-427c-a4ca-a2079d5064ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "semPath_root = \"data/output/semanticsData/\"\n",
    "semPaths = [semPath_root+p for p in os.listdir(semPath_root) if not p.startswith(\".\")]\n",
    "\n",
    "languages = [\"ASL\", \"BSL\"]\n",
    "dims = [\"50d\", \"100d\", \"200d\", \"300d\"]\n",
    "\n",
    "heightRange = range(0,100)\n",
    "tuples = product(languages, dims)\n",
    "columns = pd.MultiIndex.from_tuples(tuples, names=[\"language\", \"dim\"])\n",
    "clusterN_df = pd.DataFrame(index=heightRange, columns=columns)\n",
    "\n",
    "\"\"\"CLUSTERING -- pruning heights 0% through 100%\"\"\"\n",
    "for language in languages:\n",
    "    for dim in dims:\n",
    "        masterPath = \"results/clustering/clusterIDs/\"+language+'/'+dim+'/'\n",
    "        \n",
    "        if not os.path.exists(masterPath):\n",
    "            os.makedirs(masterPath)\n",
    "\n",
    "        p = semPath_root+language+\"_Semantics_\"+dim+\"_clean.csv.gz\"\n",
    "        print(p)\n",
    "        \n",
    "        clusterLabels_dict = {}\n",
    "        for heit in heightRange:\n",
    "            \n",
    "            if heit%25 == 0:\n",
    "                print(heit)\n",
    "                \n",
    "            signs, silhouette, clusterLabels, clusters_len = cluster_glove(p, height=heit)\n",
    "            \n",
    "            if  1 < clusters_len < len(signs):\n",
    "                clusterN_df[(language, dim)].loc[heit] = clusters_len\n",
    "                temp_dict = {sign:cluster for sign, cluster in zip(signs, clusterLabels)}\n",
    "                height_key = \"height_\"+str(heit).zfill(3)\n",
    "                clusterLabels_dict[height_key] = {}\n",
    "                clusterLabels_dict[height_key][\"clusters\"] = {\"C\"+str(key).zfill(4): [value for value, check_key in temp_dict.items() if check_key==key] for key in temp_dict.values()}\n",
    "                clusterLabels_dict[height_key][\"silhouette_score\"] = silhouette\n",
    "                \n",
    "        with open(masterPath+language+\"_\"+dim+\"_\"+\"clusterIDs.json\", \"w\") as outfile:\n",
    "             json.dump(clusterLabels_dict, outfile)\n",
    "            \n",
    "\n",
    "clusterN_df = clusterN_df.reset_index()\n",
    "clusterN_df = clusterN_df.rename(columns={\"index\":\"height\"})\n",
    "clusterN_df.to_csv(\"results/clustering/clusterN_by_height.csv\", index=True)\n",
    "clusterN_df\n",
    "\n",
    "print(\"\\n\\nCell executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138ca40-5b08-4c99-a4fd-e063471659ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"ASL\", \"BSL\"]\n",
    "dims = [\"50d\", \"100d\", \"200d\", \"300d\"]\n",
    "\n",
    "masterPath = \"results/clustering/clusterIDs/\"\n",
    "\n",
    "silhous = pd.DataFrame(columns = [\"language\", \"dim\", \"prune_height\", \"cluster_N\", \"silhouette_score\"], index=range(len(languages)*len(dims)*100))\n",
    "i = 0\n",
    "for language in languages:\n",
    "    for dim in dims:\n",
    "        read_json_path = masterPath+language+\"/\"+dim+\"/\"+language+\"_\"+dim+\"_clusterIDs.json\"\n",
    "        print(read_json_path)\n",
    "        with open(read_json_path, \"r\") as read_file:\n",
    "            clusterLabels = json.load(read_file)\n",
    "        \n",
    "        for heit in clusterLabels.keys():\n",
    "            height = int(heit.split(\"_\")[1])\n",
    "            silhous.iloc[i][\"language\"] = language\n",
    "            silhous.iloc[i][\"dim\"] = dim\n",
    "            silhous.iloc[i][\"prune_height\"] = height\n",
    "            silhous.iloc[i][\"cluster_N\"] = len(clusterLabels[heit][\"clusters\"])\n",
    "            silhous.iloc[i][\"silhouette_score\"] = clusterLabels[heit][\"silhouette_score\"]\n",
    "            i+= 1\n",
    "        \n",
    "    \n",
    "silhous = silhous.dropna()\n",
    "silhous.head()\n",
    "silhous.to_csv(\"results/clustering/clustering_silhouette_scores.csv\", index=False)\n",
    "\n",
    "print(\"\\n\\nCell executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2fe9f-aa3d-4e53-89b9-8f0343592df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the VSMs where clustering quality is highest (the higher the silhouette score the better clustering quality)\n",
    "silhous.groupby([\"language\", \"dim\"])[\"silhouette_score\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961c9ac-9427-44f4-9ba9-9488e8bf98ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "languages = [\"ASL\", \"BSL\"]\n",
    "dims = [\"50d\", \"100d\", \"200d\", \"300d\"]\n",
    "\n",
    "masterPath = \"results/clustering/clusterIDs/\"\n",
    "\n",
    "for language in languages:\n",
    "    for dim in dims:\n",
    "        read_json_path = masterPath+language+\"/\"+dim+\"/\"+language+\"_\"+dim+\"_clusterIDs.json\"\n",
    "        print(read_json_path)\n",
    "        with open(read_json_path, \"r\") as read_file:\n",
    "            clusterLabels = json.load(read_file)\n",
    "            \n",
    "        masterOutPath = \"results/clustering/signPairs_byCluster/\"+language+\"/\"+dim+\"/\"\n",
    "        if not os.path.exists(masterOutPath):\n",
    "            os.makedirs(masterOutPath)\n",
    "        \n",
    "        for heit in clusterLabels.keys():\n",
    "            height = int(heit.split(\"_\")[1])\n",
    "            \n",
    "            height_condition = 2<height<11\n",
    "            #The range above is obtained from the silhouette scores.\n",
    "            #All VSMs have the max silhouette score between heights 4 and 7.\n",
    "            #So we only look at those VSMs where cluster validity is better:\n",
    "            \n",
    "            if height_condition:\n",
    "                signPairs = {cluster: [(x,y) for (x,y) in combinations(clusterLabels[heit][\"clusters\"][cluster],2)] for cluster in clusterLabels[heit][\"clusters\"] if len(clusterLabels[heit][\"clusters\"][cluster]) > 1}\n",
    "                out_json_path = masterOutPath+language+\"_\"+dim+\"_height\"+heit+\"_signPairs_byCluster.json\"\n",
    "                with open(out_json_path,'w') as outfile:\n",
    "                    json.dump(signPairs, outfile)\n",
    "            \n",
    "print(\"\\n\\nCell executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5cf71d-d078-4d99-aa89-01c9261ce2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "700ddc66-770c-48a6-859a-a9ad48d6cd9b",
   "metadata": {},
   "source": [
    "# ***IGNORE AFTER HERE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987ea75-dc66-477c-9684-342d01760206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotOutputPath = \"plots/heights_elbow/\"\n",
    "\n",
    "# if not os.path.exists(plotOutputPath):\n",
    "#     os.makedirs(plotOutputPath)\n",
    "    \n",
    "# y_ticks = np.arange(0, 2000, 400)\n",
    "\n",
    "\n",
    "\n",
    "# for p in semPaths[:limit]:\n",
    "#     plt.figure(figsize=(12,12))\n",
    "#     print(p, \"\\n\")\n",
    "    \n",
    "#     language = p.split(\"/\")[-1].split(\"_\")[0]\n",
    "#     dim = p.split(\"/\")[-1].split(\"_\")[2]\n",
    "    \n",
    "#     heights = []\n",
    "#     for heit in range(100):\n",
    "#         clusters = cluster_glove(p, height=heit)\n",
    "#         clustersN = len(list(set([x[0] for x in clusters])))\n",
    "#         heights += [(heit, clustersN)]\n",
    "        \n",
    "#     plt.scatter([x[0] for x in heights], [x[1] for x in heights])\n",
    "#     _=plt.yticks(y_ticks)\n",
    "#     _=plt.axes().set_ylim(-100, 2000)\n",
    "#     _=plt.axes().set_xlim(-5,100)\n",
    "#     plt.savefig(plotOutputPath+language+\"_\"+dim+\".png\", dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505185f-e1f0-4fd2-b8ef-293d89e69b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22f565-d43f-4134-af08-e802643cbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd, numpy as np, os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from scipy.cluster.hierarchy import dendrogram\n",
    "# from scipy.cluster import hierarchy\n",
    "\n",
    "# distance_Tresholds = [\n",
    "#     6,\n",
    "# #                         7,\n",
    "# #                       8,9,10,\n",
    "# #                       11, 12,\n",
    "#     13, 14, 15\n",
    "    \n",
    "#                      ]\n",
    "\n",
    "# inputPath = semPaths\n",
    "# outputPath = \"../../04.Analyses/hierarchicalClustering/gloVe_VSMs_ClusteredHierarchical/\"\n",
    "# gloves = [x for x in os.listdir(inputPath) if not x.startswith(\".\")]\n",
    "\n",
    "# method = \"ward\"\n",
    "# distanceMethod = \"euclidean\"\n",
    "\n",
    "# Limit = None\n",
    "# for glove in gloves[:Limit]:\n",
    "#     datasetName = glove\n",
    "#     print(\"Now working on \", glove)\n",
    "#     data = pd.read_csv(inputPath+datasetName).set_index(\"label\")[:]\n",
    "#     print(\"length of data:\", len(data))\n",
    "#     data.head()\n",
    "#     dataOutput = data.reset_index()\n",
    "#     X = data\n",
    "#     signs = [x for x in data.index]\n",
    "#     # print(signs)\n",
    "\n",
    "#     for threshold in distance_Tresholds:\n",
    "#         model = AgglomerativeClustering(linkage= method,\n",
    "#                                         affinity= distanceMethod,\n",
    "#                                         distance_threshold=threshold,\n",
    "#                                         n_clusters=None,\n",
    "#                                         compute_distances=True\n",
    "#                                        )\n",
    "#         model_fit = model.fit(X)\n",
    "#         clusters = [\"C\"+str(c) for c in list(model.fit_predict(X))]\n",
    "#         dataOutput[\"clusters\"] = clusters\n",
    "# #             dataOutput[\"labels\"] = signs\n",
    "\n",
    "#         dataOutput.to_csv(outputPath+glove[:-4]+\"_height\"+str(threshold)+\".csv\", index=False)\n",
    "\n",
    "\n",
    "#         print(\"N of clusters: \", model_fit.n_clusters_)\n",
    "\n",
    "\n",
    "# print(\"this cell executed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
